{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe1nUEYpU0ms1WMF/zA/mS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedtarek26/Probalistic_IR/blob/main/Probalistic_IR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "OcuFz7GxXZIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UQHAMLr-W7o4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project with sample data"
      ],
      "metadata": {
        "id": "ex30NDJBXpAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK resources: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Preprocess text by tokenizing, removing stopwords, and stemming.\"\"\"\n",
        "    try:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return [stemmer.stem(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def create_sample_dataset():\n",
        "    \"\"\"Create a small sample dataset with passages, queries, and relevance judgments.\"\"\"\n",
        "    passages = {\n",
        "        \"1\": \"The cat jumped on the xylophone\",\n",
        "        \"2\": \"Cat is a noun, xylophone is a musical instrument\",\n",
        "        \"3\": \"Cats enjoy playing with musical toys\",\n",
        "        \"4\": \"Xylophones produce unique sounds\"\n",
        "    }\n",
        "    queries = {\n",
        "        \"q1\": \"cat xylophone\",\n",
        "        \"q2\": \"musical instrument\"\n",
        "    }\n",
        "    qrels = {\n",
        "        \"q1\": [\"1\", \"2\", \"3\"],\n",
        "        \"q2\": [\"2\", \"4\"]\n",
        "    }\n",
        "    return passages, queries, qrels\n",
        "\n",
        "class InvertedIndex:\n",
        "    \"\"\"A simple inverted index for storing term-passage mappings.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.index = defaultdict(list)\n",
        "        self.doc_lengths = {}\n",
        "        self.avg_doc_length = 0\n",
        "        self.total_docs = 0\n",
        "\n",
        "    def add_document(self, doc_id, text):\n",
        "        \"\"\"Add a passage to the index.\"\"\"\n",
        "        tokens = preprocess(text)\n",
        "        self.doc_lengths[doc_id] = len(tokens)\n",
        "        self.total_docs += 1\n",
        "        self.avg_doc_length = sum(self.doc_lengths.values()) / self.total_docs\n",
        "        term_counts = defaultdict(int)\n",
        "        for token in tokens:\n",
        "            term_counts[token] += 1\n",
        "        for term, freq in term_counts.items():\n",
        "            self.index[term].append((doc_id, freq))\n",
        "\n",
        "    def save(self, filename):\n",
        "        \"\"\"Save the index to a file.\"\"\"\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'index': dict(self.index),\n",
        "                    'doc_lengths': self.doc_lengths,\n",
        "                    'avg_doc_length': self.avg_doc_length,\n",
        "                    'total_docs': self.total_docs\n",
        "                }, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving index: {e}\")\n",
        "\n",
        "def bm25_score(query, index, passages, k1=1.5, b=0.75):\n",
        "    \"\"\"Compute BM25 scores for passages and return with passage text.\"\"\"\n",
        "    scores = defaultdict(float)\n",
        "    query_terms = preprocess(query)\n",
        "    N = index.total_docs\n",
        "    avgdl = index.avg_doc_length\n",
        "    for term in query_terms:\n",
        "        if term in index.index:\n",
        "            df = len(index.index[term])\n",
        "            idf = math.log(N / df) if df > 0 else 0\n",
        "            for doc_id, tf in index.index[term]:\n",
        "                score = idf * ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (index.doc_lengths[doc_id] / avgdl)) + tf)\n",
        "                scores[doc_id] += score\n",
        "    results = [(doc_id, score, passages.get(doc_id, \"Not found\")) for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
        "    return results\n",
        "\n",
        "def bim_score(query, index, passages):\n",
        "    \"\"\"Compute BIM scores for passages and return with passage text.\"\"\"\n",
        "    scores = defaultdict(float)\n",
        "    query_terms = preprocess(query)\n",
        "    N = index.total_docs\n",
        "    for term in query_terms:\n",
        "        if term in index.index:\n",
        "            df = len(index.index[term])\n",
        "            p_i = df / N if df > 0 else 0.5\n",
        "            u_i = 1 - p_i\n",
        "            c_i = math.log((p_i / u_i) * ((1 - u_i) / (1 - p_i))) if p_i > 0 and u_i > 0 else 0\n",
        "            for doc_id, _ in index.index[term]:\n",
        "                scores[doc_id] += c_i\n",
        "    results = [(doc_id, score, passages.get(doc_id, \"Not found\")) for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
        "    return results\n",
        "\n",
        "def relevance_feedback(index, query, relevant_docs, passages):\n",
        "    \"\"\"Update rankings with relevance feedback and return with passage text.\"\"\"\n",
        "    query_terms = preprocess(query)\n",
        "    N = index.total_docs\n",
        "    VR = set(relevant_docs)\n",
        "    scores = defaultdict(float)\n",
        "    for term in query_terms:\n",
        "        if term in index.index:\n",
        "            df = len(index.index[term])\n",
        "            VR_i = len([doc_id for doc_id, _ in index.index[term] if doc_id in VR])\n",
        "            p_i = (VR_i + 0.5) / (len(VR) + 1)\n",
        "            u_i = (df - VR_i + 0.5) / (N - len(VR) + 1)\n",
        "            for doc_id, tf in index.index[term]:\n",
        "                c_i = math.log(p_i / (1 - p_i) * (1 - u_i) / u_i) if p_i > 0 and p_i < 1 and u_i > 0 else 0\n",
        "                scores[doc_id] += c_i * tf\n",
        "    results = [(doc_id, score, passages.get(doc_id, \"Not found\")) for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
        "    return results\n",
        "\n",
        "def pseudo_relevance_feedback(index, query, passages, k=2):\n",
        "    \"\"\"Apply pseudo-relevance feedback using top k passages.\"\"\"\n",
        "    initial_ranking = bm25_score(query, index, passages)[:k]\n",
        "    relevant_docs = [doc_id for doc_id, _, _ in initial_ranking]\n",
        "    return relevance_feedback(index, query, relevant_docs, passages)\n",
        "\n",
        "def evaluate_system(index, queries, qrels):\n",
        "    \"\"\"Evaluate the system using Mean Average Precision (MAP).\"\"\"\n",
        "    map_score = 0\n",
        "    for query_id, query in queries.items():\n",
        "        ranking = bm25_score(query, index, passages={})  # Passages not needed for MAP\n",
        "        relevant_docs = set(qrels.get(query_id, []))\n",
        "        relevant_retrieved = 0\n",
        "        precision_sum = 0\n",
        "        for i, (doc_id, _, _) in enumerate(ranking, 1):\n",
        "            if doc_id in relevant_docs:\n",
        "                relevant_retrieved += 1\n",
        "                precision_sum += relevant_retrieved / i\n",
        "        avg_precision = precision_sum / len(relevant_docs) if relevant_docs else 0\n",
        "        map_score += avg_precision\n",
        "    return map_score / len(queries) if queries else 0\n",
        "\n",
        "def print_ranking(title, ranking):\n",
        "    \"\"\"Print ranking results in a formatted way.\"\"\"\n",
        "    print(f\"\\n{title}:\")\n",
        "    if not ranking:\n",
        "        print(\"No results found.\")\n",
        "        return\n",
        "    for i, (doc_id, score, text) in enumerate(ranking, 1):\n",
        "        print(f\"{i}. Passage ID: {doc_id}, Score: {score:.4f}\")\n",
        "        print(f\"   Text: {text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load sample dataset\n",
        "    print(\"Loading sample dataset...\")\n",
        "    passages, queries, qrels = create_sample_dataset()\n",
        "    print(f\"Loaded {len(passages)} passages and {len(queries)} queries.\")\n",
        "\n",
        "    # Build the index\n",
        "    index = InvertedIndex()\n",
        "    for passage_id, text in passages.items():\n",
        "        index.add_document(passage_id, text)\n",
        "    index.save(\"index.json\")\n",
        "    print(\"Index saved to 'index.json'.\")\n",
        "\n",
        "    # Prompt user for query\n",
        "    while True:\n",
        "        sample_query = input(\"\\nEnter your query (or 'quit' to exit): \").strip()\n",
        "        if sample_query.lower() == 'quit':\n",
        "            print(\"Exiting program.\")\n",
        "            break\n",
        "        if not sample_query:\n",
        "            print(\"Error: Query cannot be empty. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing query: {sample_query}\")\n",
        "\n",
        "        # BM25 ranking\n",
        "        bm25_ranking = bm25_score(sample_query, index, passages)\n",
        "        print_ranking(\"BM25 Ranking\", bm25_ranking)\n",
        "\n",
        "        # BIM ranking\n",
        "        bim_ranking = bim_score(sample_query, index, passages)\n",
        "        print_ranking(\"BIM Ranking\", bim_ranking)\n",
        "\n",
        "        # Relevance feedback (assume top BM25 result is relevant)\n",
        "        if bm25_ranking:\n",
        "            feedback_ranking = relevance_feedback(index, sample_query, [bm25_ranking[0][0]], passages)\n",
        "            print_ranking(\"Relevance Feedback Ranking\", feedback_ranking)\n",
        "\n",
        "        # Pseudo-relevance feedback\n",
        "        pseudo_ranking = pseudo_relevance_feedback(index, sample_query, passages)\n",
        "        print_ranking(\"Pseudo-Relevance Feedback Ranking\", pseudo_ranking)\n",
        "\n",
        "    # Evaluate with MAP for predefined queries\n",
        "    map_score = evaluate_system(index, queries, qrels)\n",
        "    print(f\"\\nMAP Score for predefined queries: {map_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSC_KXySXWKg",
        "outputId": "00c67363-1112-4dac-8261-f82a768642d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sample dataset...\n",
            "Loaded 4 passages and 2 queries.\n",
            "Index saved to 'index.json'.\n",
            "\n",
            "Enter your query (or 'quit' to exit): cat\n",
            "\n",
            "Processing query: cat\n",
            "\n",
            "BM25 Ranking:\n",
            "1. Passage ID: 1, Score: 0.3316\n",
            "   Text: The cat jumped on the xylophone\n",
            "2. Passage ID: 2, Score: 0.2665\n",
            "   Text: Cat is a noun, xylophone is a musical instrument\n",
            "3. Passage ID: 3, Score: 0.2665\n",
            "   Text: Cats enjoy playing with musical toys\n",
            "\n",
            "BIM Ranking:\n",
            "1. Passage ID: 1, Score: 2.1972\n",
            "   Text: The cat jumped on the xylophone\n",
            "2. Passage ID: 2, Score: 2.1972\n",
            "   Text: Cat is a noun, xylophone is a musical instrument\n",
            "3. Passage ID: 3, Score: 2.1972\n",
            "   Text: Cats enjoy playing with musical toys\n",
            "\n",
            "Relevance Feedback Ranking:\n",
            "1. Passage ID: 1, Score: 0.5878\n",
            "   Text: The cat jumped on the xylophone\n",
            "2. Passage ID: 2, Score: 0.5878\n",
            "   Text: Cat is a noun, xylophone is a musical instrument\n",
            "3. Passage ID: 3, Score: 0.5878\n",
            "   Text: Cats enjoy playing with musical toys\n",
            "\n",
            "Pseudo-Relevance Feedback Ranking:\n",
            "1. Passage ID: 1, Score: 1.6094\n",
            "   Text: The cat jumped on the xylophone\n",
            "2. Passage ID: 2, Score: 1.6094\n",
            "   Text: Cat is a noun, xylophone is a musical instrument\n",
            "3. Passage ID: 3, Score: 1.6094\n",
            "   Text: Cats enjoy playing with musical toys\n",
            "\n",
            "Enter your query (or 'quit' to exit): quit\n",
            "Exiting program.\n",
            "\n",
            "MAP Score for predefined queries: 0.7083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project with [20 Newsgroups](https://www.kaggle.com/datasets/crawford/20-newsgroups) Dataset\n",
        "\n",
        "This is a list of the 20 newsgroups:\n",
        "\n",
        "- comp.graphics\n",
        "- comp.os.ms-windows.misc\n",
        "- comp.sys.ibm.pc.hardware\n",
        "- comp.sys.mac.hardware\n",
        "- comp.windows.x rec.autos\n",
        "- rec.motorcycles\n",
        "- rec.sport.baseball\n",
        "- rec.sport.hockey sci.crypt\n",
        "- sci.electronics\n",
        "- sci.med\n",
        "- sci.space\n",
        "- misc.forsale talk.politics.misc\n",
        "- talk.politics.guns\n",
        "- talk.politics.mideast talk.religion.misc\n",
        "- alt.atheism\n",
        "- soc.religion.christian"
      ],
      "metadata": {
        "id": "KwIpzKL2XyqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK resources: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Preprocess input text for indexing or querying:\n",
        "    - Tokenize\n",
        "    - Lowercase\n",
        "    - Remove stopwords and non-alphanumeric tokens\n",
        "    - Apply stemming\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return [stemmer.stem(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def load_20newsgroups_data(limit=100):\n",
        "    \"\"\"\n",
        "    Load a subset of the 20 Newsgroups dataset and prepare:\n",
        "    - A mapping from doc ID to document text (passages)\n",
        "    - Synthetic queries\n",
        "    - Query relevance judgments (qrels)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch only the categories needed for our example queries\n",
        "        news = fetch_20newsgroups(\n",
        "            subset='all',\n",
        "            #categories=['sci.space', 'comp.graphics'],\n",
        "            remove=('headers', 'footers', 'quotes')\n",
        "        )\n",
        "        passages = {}\n",
        "        for i, doc in enumerate(news.data[:limit]):  # Limit for performance\n",
        "            if doc.strip():  # Skip empty documents\n",
        "                passages[str(i)] = doc.strip()\n",
        "\n",
        "        # Generate synthetic queries and qrels based on categories\n",
        "        queries = {\n",
        "            \"q1\": \"space exploration\",\n",
        "            \"q2\": \"computer graphics\"\n",
        "        }\n",
        "        qrels = defaultdict(list)\n",
        "        label_map = {i: cat for i, cat in enumerate(news.target_names)}\n",
        "        for i, label in enumerate(news.target[:limit]):\n",
        "            if str(i) in passages:\n",
        "                category = label_map.get(label, \"\")\n",
        "                if category == \"sci.space\":\n",
        "                    qrels[\"q1\"].append(str(i))\n",
        "                elif category == \"comp.graphics\":\n",
        "                    qrels[\"q2\"].append(str(i))\n",
        "\n",
        "        return passages, queries, qrels\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading 20 Newsgroups data: {e}\")\n",
        "        return {}, {}, defaultdict(list)\n",
        "\n",
        "class InvertedIndex:\n",
        "    \"\"\"\n",
        "    A basic inverted index supporting BM25/BIM scoring.\n",
        "    Stores term frequencies per document and global collection statistics.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.index = defaultdict(list)\n",
        "        self.doc_lengths = {}\n",
        "        self.avg_doc_length = 0\n",
        "        self.total_docs = 0\n",
        "\n",
        "    def add_document(self, doc_id, text):\n",
        "        \"\"\"\n",
        "        Preprocess and index a single document.\n",
        "        Update average document length and total document count.\n",
        "        \"\"\"\n",
        "        tokens = preprocess(text)\n",
        "        self.doc_lengths[doc_id] = len(tokens)\n",
        "        self.total_docs += 1\n",
        "        self.avg_doc_length = sum(self.doc_lengths.values()) / self.total_docs\n",
        "        term_counts = defaultdict(int)\n",
        "        for token in tokens:\n",
        "            term_counts[token] += 1\n",
        "        for term, freq in term_counts.items():\n",
        "            self.index[term].append((doc_id, freq))\n",
        "\n",
        "    def save(self, filename):\n",
        "        \"\"\"\n",
        "        Serialize the index and statistics to disk.\n",
        "        Useful for reusing the index without reprocessing.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'index': dict(self.index),\n",
        "                    'doc_lengths': self.doc_lengths,\n",
        "                    'avg_doc_length': self.avg_doc_length,\n",
        "                    'total_docs': self.total_docs\n",
        "                }, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving index: {e}\")\n",
        "\n",
        "def bm25_score(query, index, passages, k1=1.5, b=0.75):\n",
        "    \"\"\"\n",
        "    Score documents using the BM25 formula.\n",
        "    - Accounts for term frequency and document length normalization.\n",
        "    - Returns top documents with their scores and snippets.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(float)\n",
        "    query_terms = preprocess(query)\n",
        "    N = index.total_docs\n",
        "    avgdl = index.avg_doc_length\n",
        "    for term in query_terms:\n",
        "        if term in index.index:\n",
        "            df = len(index.index[term])\n",
        "            idf = math.log(N / df) if df > 0 else 0\n",
        "            for doc_id, tf in index.index[term]:\n",
        "                score = idf * ((k1 + 1) * tf) / (k1 * ((1 - b) + b * (index.doc_lengths[doc_id] / avgdl)) + tf)\n",
        "                scores[doc_id] += score\n",
        "    # Return top 5 results with truncated text (first 100 chars) for readability\n",
        "    results = [(doc_id, score, passages.get(doc_id, \"Not found\")[:100] + \"...\")\n",
        "               for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)][:5]\n",
        "    return results\n",
        "\n",
        "def bim_score(query, index, passages):\n",
        "    \"\"\"\n",
        "    Binary Independence Model (BIM) scoring based on term presence.\n",
        "    - Uses odds ratio of term in relevant vs. non-relevant documents.\n",
        "    - Assumes binary term presence and independence across terms.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(float)\n",
        "    query_terms = preprocess(query)\n",
        "    N = index.total_docs\n",
        "    for term in query_terms:\n",
        "        if term in index.index:\n",
        "            df = len(index.index[term])\n",
        "            p_i = df / N if df > 0 else 0.5\n",
        "            u_i = 1 - p_i\n",
        "            c_i = math.log((p_i / u_i) * ((1 - u_i) / (1 - p_i))) if p_i > 0 and u_i > 0 else 0\n",
        "            for doc_id, _ in index.index[term]:\n",
        "                scores[doc_id] += c_i\n",
        "    results = [(doc_id, score, passages.get(doc_id, \"Not found\")[:100] + \"...\")\n",
        "               for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)][:5]\n",
        "    return results\n",
        "\n",
        "def relevance_feedback(index, query, relevant_docs, passages):\n",
        "    \"\"\"\n",
        "    Update term weights using relevance feedback (Rocchio-style logic).\n",
        "    - Estimate new term relevance probabilities based on user-labeled docs.\n",
        "    - Re-score documents accordingly.\n",
        "    \"\"\"\n",
        "    query_terms = preprocess(query)\n",
        "    N = index.total_docs\n",
        "    VR = set(relevant_docs)\n",
        "    scores = defaultdict(float)\n",
        "    for term in query_terms:\n",
        "        if term in index.index:\n",
        "            df = len(index.index[term])\n",
        "            VR_i = len([doc_id for doc_id, _ in index.index[term] if doc_id in VR])\n",
        "            p_i = (VR_i + 0.5) / (len(VR) + 1)\n",
        "            u_i = (df - VR_i + 0.5) / (N - len(VR) + 1)\n",
        "            for doc_id, tf in index.index[term]:\n",
        "                c_i = math.log(p_i / (1 - p_i) * (1 - u_i) / u_i) if p_i > 0 and p_i < 1 and u_i > 0 else 0\n",
        "                scores[doc_id] += c_i * tf\n",
        "    results = [(doc_id, score, passages.get(doc_id, \"Not found\")[:100] + \"...\")\n",
        "               for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)][:5]\n",
        "    return results\n",
        "\n",
        "def pseudo_relevance_feedback(index, query, passages, k=2):\n",
        "    \"\"\"\n",
        "    Apply pseudo-relevance feedback:\n",
        "    - Assume top-k documents from BM25 are relevant.\n",
        "    - Re-compute term weights using estimated relevance.\n",
        "    \"\"\"\n",
        "    initial_ranking = bm25_score(query, index, passages)[:k]\n",
        "    relevant_docs = [doc_id for doc_id, _, _ in initial_ranking]\n",
        "    return relevance_feedback(index, query, relevant_docs, passages)\n",
        "\n",
        "def evaluate_system(index, queries, qrels):\n",
        "    \"\"\"\n",
        "    Evaluate the ranking system using Mean Average Precision (MAP):\n",
        "    - For each query, compute precision at each relevant doc.\n",
        "    - Average over all queries.\n",
        "    \"\"\"\n",
        "    map_score = 0\n",
        "    for query_id, query in queries.items():\n",
        "        ranking = bm25_score(query, index, passages={})  # Passages not needed for MAP\n",
        "        relevant_docs = set(qrels.get(query_id, []))\n",
        "        relevant_retrieved = 0\n",
        "        precision_sum = 0\n",
        "        for i, (doc_id, _, _) in enumerate(ranking, 1):\n",
        "            if doc_id in relevant_docs:\n",
        "                relevant_retrieved += 1\n",
        "                precision_sum += relevant_retrieved / i\n",
        "        avg_precision = precision_sum / len(relevant_docs) if relevant_docs else 0\n",
        "        map_score += avg_precision\n",
        "    return map_score / len(queries) if queries else 0\n",
        "\n",
        "def print_ranking(title, ranking):\n",
        "    \"\"\"Format and display ranking results with score and truncated passage.\"\"\"\n",
        "    print(f\"\\n{title}:\")\n",
        "    if not ranking:\n",
        "        print(\"No results found.\")\n",
        "        return\n",
        "    for i, (doc_id, score, text) in enumerate(ranking, 1):\n",
        "        print(f\"{i}. Document ID: {doc_id}, Score: {score:.4f}\")\n",
        "        print(f\"   Text: {text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load 20 Newsgroups dataset\n",
        "    print(\"Loading 20 Newsgroups dataset...\")\n",
        "    passages, queries, qrels = load_20newsgroups_data(limit=100)\n",
        "    if not passages:\n",
        "        print(\"Failed to load dataset. Exiting.\")\n",
        "        sys.exit(1)\n",
        "    print(f\"Loaded {len(passages)} documents and {len(queries)} queries.\")\n",
        "\n",
        "    # Build the index\n",
        "    index = InvertedIndex()\n",
        "    for doc_id, text in passages.items():\n",
        "        index.add_document(doc_id, text)\n",
        "    index.save(\"index.json\")\n",
        "    print(\"Index saved to 'index.json'.\")\n",
        "\n",
        "    # Prompt user for query\n",
        "    print(\"\\nSample queries: 'space exploration', 'computer graphics'\")\n",
        "    with open(\"results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        while True:\n",
        "            sample_query = input(\"\\nEnter your query (or 'quit' to exit): \").strip()\n",
        "            if sample_query.lower() == 'quit':\n",
        "                print(\"Exiting program.\")\n",
        "                break\n",
        "            if not sample_query:\n",
        "                print(\"Error: Query cannot be empty. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing query: {sample_query}\")\n",
        "            f.write(f\"\\nQuery: {sample_query}\\n\")\n",
        "\n",
        "            # BM25 ranking\n",
        "            bm25_ranking = bm25_score(sample_query, index, passages)\n",
        "            print_ranking(\"BM25 Ranking\", bm25_ranking)\n",
        "            f.write(\"\\nBM25 Ranking:\\n\")\n",
        "            for doc_id, score, text in bm25_ranking:\n",
        "                f.write(f\"Document ID: {doc_id}, Score: {score:.4f}, Text: {text}\\n\")\n",
        "\n",
        "            # BIM ranking\n",
        "            bim_ranking = bim_score(sample_query, index, passages)\n",
        "            print_ranking(\"BIM Ranking\", bim_ranking)\n",
        "            f.write(\"\\nBIM Ranking:\\n\")\n",
        "            for doc_id, score, text in bim_ranking:\n",
        "                f.write(f\"Document ID: {doc_id}, Score: {score:.4f}, Text: {text}\\n\")\n",
        "\n",
        "            # Relevance feedback\n",
        "            if bm25_ranking:\n",
        "                feedback_ranking = relevance_feedback(index, sample_query, [bm25_ranking[0][0]], passages)\n",
        "                print_ranking(\"Relevance Feedback Ranking\", feedback_ranking)\n",
        "                f.write(\"\\nRelevance Feedback Ranking:\\n\")\n",
        "                for doc_id, score, text in feedback_ranking:\n",
        "                    f.write(f\"Document ID: {doc_id}, Score: {score:.4f}, Text: {text}\\n\")\n",
        "\n",
        "            # Pseudo-relevance feedback\n",
        "            pseudo_ranking = pseudo_relevance_feedback(index, sample_query, passages)\n",
        "            print_ranking(\"Pseudo-Relevance Feedback Ranking\", pseudo_ranking)\n",
        "            f.write(\"\\nPseudo-Relevance Feedback Ranking:\\n\")\n",
        "            for doc_id, score, text in pseudo_ranking:\n",
        "                f.write(f\"Document ID: {doc_id}, Score: {score:.4f}, Text: {text}\\n\")\n",
        "\n",
        "    # Evaluate with MAP for predefined queries\n",
        "    map_score = evaluate_system(index, queries, qrels)\n",
        "    print(f\"\\nMAP Score for predefined queries: {map_score:.4f}\")\n",
        "    with open(\"results.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"\\nMAP Score for predefined queries: {map_score:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aqisihtXtl1",
        "outputId": "ba52ca6d-8d08-4b3a-91c3-70d2e455b03f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20 Newsgroups dataset...\n",
            "Loaded 99 documents and 2 queries.\n",
            "Index saved to 'index.json'.\n",
            "\n",
            "Sample queries: 'space exploration', 'computer graphics'\n",
            "\n",
            "Enter your query (or 'quit' to exit): graphics\n",
            "\n",
            "Processing query: graphics\n",
            "\n",
            "BM25 Ranking:\n",
            "1. Document ID: 22, Score: 5.2477\n",
            "   Text: I don't have nor Imagine nor Real 3d, but as old\n",
            "Amiga user I think you should take a look also to \n",
            "...\n",
            "2. Document ID: 1, Score: 4.9351\n",
            "   Text: My brother is in the market for a high-performance video card that supports\n",
            "VESA local bus with 1-2M...\n",
            "3. Document ID: 93, Score: 4.5491\n",
            "   Text: A friend and I have ATI Graphic Ultra display adaptors, and they have\n",
            "been reasonably good performer...\n",
            "\n",
            "BIM Ranking:\n",
            "1. Document ID: 1, Score: -6.9315\n",
            "   Text: My brother is in the market for a high-performance video card that supports\n",
            "VESA local bus with 1-2M...\n",
            "2. Document ID: 22, Score: -6.9315\n",
            "   Text: I don't have nor Imagine nor Real 3d, but as old\n",
            "Amiga user I think you should take a look also to \n",
            "...\n",
            "3. Document ID: 93, Score: -6.9315\n",
            "   Text: A friend and I have ATI Graphic Ultra display adaptors, and they have\n",
            "been reasonably good performer...\n",
            "\n",
            "Relevance Feedback Ranking:\n",
            "1. Document ID: 22, Score: 9.5037\n",
            "   Text: I don't have nor Imagine nor Real 3d, but as old\n",
            "Amiga user I think you should take a look also to \n",
            "...\n",
            "2. Document ID: 1, Score: 4.7519\n",
            "   Text: My brother is in the market for a high-performance video card that supports\n",
            "VESA local bus with 1-2M...\n",
            "3. Document ID: 93, Score: 4.7519\n",
            "   Text: A friend and I have ATI Graphic Ultra display adaptors, and they have\n",
            "been reasonably good performer...\n",
            "\n",
            "Pseudo-Relevance Feedback Ranking:\n",
            "1. Document ID: 22, Score: 11.5470\n",
            "   Text: I don't have nor Imagine nor Real 3d, but as old\n",
            "Amiga user I think you should take a look also to \n",
            "...\n",
            "2. Document ID: 1, Score: 5.7735\n",
            "   Text: My brother is in the market for a high-performance video card that supports\n",
            "VESA local bus with 1-2M...\n",
            "3. Document ID: 93, Score: 5.7735\n",
            "   Text: A friend and I have ATI Graphic Ultra display adaptors, and they have\n",
            "been reasonably good performer...\n",
            "\n",
            "Enter your query (or 'quit' to exit): quit\n",
            "Exiting program.\n",
            "\n",
            "MAP Score for predefined queries: 0.3611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1uL3Q_vvYXRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}